{
    "0zxugBcgF5": {
        "inference_review": "**Summary**\nThe paper investigates the overoptimization problem in Reinforcement Learning from Human Feedback (RLHF) by introducing a method called Off-Policy Corrected Reward Modeling (OCRM). It aims to address the distribution shift that arises when the reward model (RM) is trained on responses from the Supervised Fine-Tuning (SFT) model but evaluated on the Reinforcement Learning (RL) model, causing the RM to overestimate the reward. The authors propose using importance weighting to correct this shift, providing a theoretical guarantee for the estimation error and empirical results that demonstrate the effectiveness of the method. However, the paper has been critiqued for its lack of clarity in explaining the method, its theoretical contributions, and its empirical results.\n\n**Strengths**\n- The paper is well-written and easy to follow, with a clear presentation of the problem and its solution.\n- The proposed method is straightforward, simple, and easy to implement, making it a practical approach for addressing overoptimization in RLHF.\n- The paper provides both theoretical and empirical analysis, which is beneficial for understanding the method's effectiveness and limitations.\n- The approach of using importance weighting to address the distribution shift between the SFT model and the RL model is novel and interesting.\n\n**Weaknesses**\n- The paper lacks clarity in its methodological explanation, particularly in the use of importance weights and their impact on the RM's performance.\n- There is a significant gap in the theoretical contributions, as the paper does not provide a novel theoretical perspective but rather a straightforward application of existing concepts.\n- The experimental results are not compelling, showing limited improvements over existing methods and not fully validating the theoretical claims.\n- The paper does not sufficiently address the potential negative impacts of the proposed method, such as overfitting or the effects of importance weighting on the model's generalization capabilities.\n- The method's performance is highly dependent on the hyperparameter $k$, and the paper does not adequately explore the sensitivity of the method to this parameter.\n- The paper's use of a single model (Pythia-1B) for all experiments and the limited number of training iterations used are not well justified, raising concerns about the generalizability and robustness of the findings.\n- The discussion on the RM's accuracy and the comparison with other methods like RLP-SPG and WPO are insufficiently explored, which could potentially undermine the validity of the proposed method.\n\n**Questions**\n- Could the authors clarify the method's implementation, particularly the use of importance weights in RM training and the impact of $k$ on the results?\n- How does the method's performance vary with different values of $k$ and how does it compare to other methods like RLP-SPG and WPO?\n- Can the authors provide more detailed experimental results, including additional training iterations and using multiple models, to better validate the theoretical claims?\n- How does the proposed method handle the potential negative impacts such as overfitting and the effects of importance weighting on the model's generalization capabilities?\n- What are the limitations of the proposed method, and how can they be addressed to improve the method's effectiveness and robustness?\n- Can the authors discuss the choice of using a single model for all experiments and the implications of this choice on the generalizability of the findings?"
    },
    "2H85485yAb": {
        "inference_review": "**Summary**\nThe paper investigates the internal representations of large language models (LLMs) to understand the reasoning process behind their truth-value judgments. By analyzing various probing methods including logistic regression, contrast consistent search, and mass mean probing, the study attempts to identify the latent dimensions that correlate with truth values. The findings suggest that these dimensions are not only sensitive to the context but also influenced by irrelevant information, potentially leading to inaccuracies in truth-value judgments. The paper further explores how these dimensions causally mediate the truth-value judgments, though it lacks empirical backing for this claim. The research also introduces a new probing method called Contrast Consistent Regularization (CCR), which is shown to perform similarly to existing methods but with more stable convergence.\n\n**Strengths**\n- The paper is well-written, with a clear and logical structure that makes it easy to follow.\n- The experimental design is robust, covering various models, layers, and probing methods, which increases the generalizability of the results.\n- The introduction of a new probing method, Contrast Consistent Regularization (CCR), which performs comparably to existing methods and shows more stable convergence.\n- The study provides a comprehensive analysis of the latent dimensions that correlate with truth values, exploring their sensitivity to context and irrelevant information.\n- The paper's findings are well-supported by data and well-presented, making the results accessible and understandable.\n- The topic of understanding the internal representations of LLMs is both important and timely, contributing to the ongoing discussions in the field.\n\n**Weaknesses**\n- The paper's methodology for analyzing the causal relationship between premise and hypothesis is not clearly defined or justified, which makes the conclusions drawn from this analysis questionable.\n- The paper does not sufficiently address the limitations of its findings, particularly in terms of the model size and the diversity of datasets used.\n- There is a lack of exploration into the potential impacts of instruction tuning on the internal representations of LLMs, which is a significant aspect of current LLM development.\n- The paper does not provide enough empirical evidence to support its claims about the causal mediation of latent dimensions in truth-value judgments.\n- The discussion section is brief and does not adequately summarize the main findings or discuss the implications of the study.\n- The novelty of the research is limited, as it primarily extends the work of previous studies such as Marks and Tegmark (2024), without introducing new insights or methodologies.\n\n**Questions**\n- Can the authors clarify the methodology used to analyze the causal relationship between premise and hypothesis? Specifically, what statistical methods were employed, and how were the error scores computed?\n- How does the paper account for the potential effects of instruction tuning on the internal representations of LLMs? Could the authors provide additional experiments or analysis to address this aspect?\n- In what ways can the study be extended to include a wider range of model sizes and datasets to better understand the generalizability of the findings?\n- Could the authors elaborate on the limitations of the study, particularly regarding the potential impacts of model size and dataset diversity on the results?\n- How do the authors justify the claim about the causal mediation of latent dimensions in truth-value judgments, and what empirical evidence supports this claim?\n- What are the main takeaways from this study, and how do they contribute to the current understanding of LLMs' internal representations and their truth-value judgments?"
    },
    "2YdSsi0bxK": {
        "inference_review": "**Summary**\nThe paper introduces a novel approach to selectively unlearn copyrighted content from large language models (LLMs) to prevent copyright infringement. This is achieved by employing a selective unlearning framework, termed SUV, which employs a sliding window mechanism to identify memorized passages, and utilizes Direct Preference Optimization (DPO) to minimize the influence of memorized content on unrelated tasks. The framework also incorporates gradient projection and Fisher information regularization to ensure that unlearning does not significantly degrade the model's performance on other tasks. Extensive experiments demonstrate the effectiveness of SUV in reducing the risk of copyright infringement while preserving the utility of the model. However, the paper is criticized for its lack of clarity in methodological details, its limited evaluation scope, and potential concerns about the practical applicability and scalability of its proposed methods.\n\n**Strengths**\n- The paper addresses the significant and timely issue of copyright infringement in Large Language Models (LLMs), which is critical for the responsible development and deployment of AI technology.\n- The proposed framework, SUV, is both novel and well-structured, providing a clear and logical flow that outlines its components and functionalities.\n- Extensive experiments were conducted to demonstrate the effectiveness of SUV in reducing copyright infringement while preserving the model's utility.\n- The paper is well-written, making it easy for readers to understand the complex concepts and methods involved.\n- The methodology behind the proposed approach is sound, particularly the use of DPO to minimize the influence of memorized content on unrelated tasks.\n\n**Weaknesses**\n- The paper lacks clarity in some methodological details, particularly the sliding window mechanism and the selection of parameters in the DPO training process.\n- The evaluation scope is limited, primarily focusing on small datasets and not adequately addressing the scalability and practical applicability of SUV in real-world scenarios.\n- The paper does not sufficiently discuss the potential ethical implications of the proposed method, particularly the impact on the model's ability to generate creative content and the potential for unintended consequences.\n- There are concerns about the potential for the model to still generate copyrighted content if the unlearning process is not fully effective.\n- The experimental setup is not comprehensive, as it lacks a comparison with state-of-the-art methods, and the baseline models used are not clearly defined.\n- There are concerns about the practicality of the proposed method, especially in terms of its scalability and the computational resources required for the training process.\n\n**Questions**\n- Could you clarify the exact mechanism and parameters used in the sliding window mechanism?\n- How is the parameter selection process for the DPO training determined, and how are the hyperparameters tuned?\n- Can you provide more details on the experimental setup, including the baseline models used and the metrics employed to evaluate the performance of SUV?\n- How does SUV perform on larger and more diverse datasets, and what are the results when compared to state-of-the-art methods?\n- How do you address the potential ethical implications of the method, such as its impact on the model's ability to generate creative content and the potential for unintended consequences?\n- What measures can be taken to ensure the full effectiveness of the unlearning process and minimize the risk of the model generating copyrighted content?"
    },
    "BM192Ps5Nv": {
        "inference_review": "**Summary**\nThe paper delves into the quantization of reasoning language models, focusing on various configurations and their impacts on model performance. It presents a systematic evaluation of quantization methods across multiple reasoning models, including the DeepSeek-R1-Distill-Qwen and LLAMA families, from 1.5B to 70B parameters. The study examines weight-only, weight-activation, and KV cache quantization using state-of-the-art algorithms at different bit-widths and across diverse benchmarks. Key findings highlight the need for larger models and longer reasoning steps for improved performance, although certain configurations like W4A4KV4 show significant accuracy risks. The paper also touches upon how quantization affects the output lengths of reasoning models, though this aspect is noted as underexplored.\n\n**Strengths**\n- The paper conducts a thorough investigation into the effects of quantization on reasoning models, covering a wide range of configurations and models, which enhances the understanding of this complex issue.\n- The study is well-motivated, providing detailed results on the impact of quantization on both reasoning models and their performance on various benchmarks.\n- The paper is clearly written, making it accessible to readers and facilitating the understanding of the subject matter.\n- The research is original, as it presents new insights into the quantization of reasoning models, a topic not extensively explored in existing literature.\n- The evaluation is comprehensive, including both model size and task difficulty, which helps in identifying significant factors affecting the performance of quantized models.\n- The paper includes a detailed discussion on the effects of quantization on the output length of reasoning models, adding to the depth of the analysis.\n\n**Weaknesses**\n- The paper lacks a detailed comparison with existing methods, particularly recent advancements in quantization techniques.\n- There is insufficient discussion on the underlying mechanisms behind the observed effects, such as the impact of quantization on the output length and the reasons for the performance differences in various model configurations.\n- The paper does not sufficiently explore the trade-offs between accuracy and latency, a crucial aspect for practical applications.\n- The inclusion of additional baselines and a broader range of quantization configurations could enhance the robustness and applicability of the findings.\n- The paper has several inconsistencies and potential errors in figures and tables, which could lead to misunderstandings about the results presented.\n- There is a need for more in-depth analysis of the findings, particularly the observations related to the effects of quantization on model size and task difficulty.\n\n**Questions**\n- Can you provide more detailed comparisons with recent quantization methods, such as those mentioned in Kurtic et al. (2023)?\n- Why does the model size and task difficulty have a significant impact on the performance of quantized models?\n- Could you clarify the reasons for the observed discrepancies in performance between different quantization configurations?\n- How does the quantization affect the output length of reasoning models, and what are the underlying mechanisms?\n- What are the specific effects of quantization on the output length and the reasoning steps of the models?\n- Is there a potential for reducing the latency of reasoning models through quantization, and what are the trade-offs in terms of accuracy?\n- Could you provide more details on the experimental setup, including the specific quantization algorithms and parameters used, to better understand the results presented?"
    },
    "exW2SFJK4H": {
        "inference_review": "**Summary**\nThe paper introduces a dynamic evaluation framework for assessing unlearning in large language models (LLMs). This framework utilizes a knowledge graph constructed from the model's pre-unlearning outputs, which allows for the automatic generation of structured single-hop, multi-hop, and alias-based queries. The evaluation process is designed to be dynamic and scalable, providing a robust and adaptable method for assessing unlearning effectiveness. The methodology is supported by experimental results that demonstrate its effectiveness and relevance, highlighting both the strengths and limitations of the approach. The framework is proposed as a practical solution for evaluating the robustness of unlearning methods, addressing the need for more rigorous and dynamic testing in the field.\n\n**Strengths**\n- The paper is well-written, clear, and easy to follow, making it accessible to a wide range of readers.\n- The introduction of a dynamic framework for evaluating unlearning in large language models (LLMs) is novel and timely, as it addresses the critical need for robust and scalable evaluation methods.\n- The methodology is thoroughly explained, with a clear description of how the framework is constructed and utilized, which enhances its reproducibility and applicability.\n- The paper's focus on unlearning in LLMs is relevant and significant, highlighting the importance of understanding and mitigating potential negative impacts of AI systems on society.\n- The experimental results are robust, demonstrating the framework's effectiveness in uncovering new failure modes and enhancing the evaluation of unlearning methods.\n- The framework's use of a knowledge graph constructed from pre-unlearning outputs is an innovative approach that allows for automatic generation of structured queries, enhancing the efficiency and scalability of the evaluation process.\n\n**Weaknesses**\n- The paper lacks a detailed discussion on the potential biases and limitations of the dynamic framework, particularly concerning the reliance on the model's own knowledge structure for constructing the knowledge graph.\n- The use of multi-hop queries as a primary method for evaluating unlearning effectiveness may not fully capture the complexity of real-world interactions, as these queries are often simplified or artificially constructed.\n- The methodology's dependency on specific model architectures (e.g., transformer-based models) might limit its applicability to other types of models, such as GPT models.\n- The paper does not sufficiently explore the potential negative impacts of the proposed framework, such as the unintended removal of beneficial knowledge or the potential for biased outcomes due to the model's knowledge structure.\n- The evaluation metrics used are somewhat simplistic and may not fully reflect the nuanced capabilities of LLMs in real-world scenarios.\n\n**Questions**\n- Could the authors provide more detailed insights into how the knowledge graph is constructed and how it affects the results, particularly concerning the potential biases and limitations of the method?\n- How do the authors address the potential issues related to the removal of beneficial knowledge or biased outcomes due to the model's knowledge structure?\n- What are the specific challenges and limitations of applying the proposed framework to other types of models, such as GPT models?\n- Can the authors elaborate on how the framework handles more complex, real-world interactions, and what potential negative impacts might arise from its use?\n- How do the authors justify the use of multi-hop queries as the primary method for evaluating unlearning effectiveness, and could they provide examples of how this might not fully capture the complexity of real-world interactions?"
    },
    "fQcUZMPIvu": {
        "inference_review": "**Summary**\nThe paper introduces AgentRewardBench, a benchmark aimed at evaluating the performance of Large Language Models (LLMs) as judges for web agents' trajectories. The benchmark consists of over 1300 trajectories across various web environments, each annotated by human experts to assess the success, side effects, and repetitiveness of web agents' actions. This evaluation framework is critical for understanding the effectiveness of LLM judges in automatically assessing web agents' performance, which could potentially streamline evaluation processes in the future. The authors also provide an analysis of various LLM judges against the benchmark, highlighting their strengths and weaknesses, and pointing out discrepancies between expert annotations and rule-based evaluations commonly used in other benchmarks.\n\n**Strengths**\n- The paper introduces AgentRewardBench, a comprehensive and diverse benchmark for evaluating the performance of Large Language Models (LLMs) as judges for web agents' trajectories, which is a significant contribution to the field.\n- The benchmark is meticulously designed, involving over 1300 trajectories from multiple web environments, each carefully annotated by human experts to assess the success, side effects, and repetitiveness of web agents' actions.\n- The evaluation of various LLM judges against this benchmark provides valuable insights into their strengths and weaknesses, which could guide future research and development in this area.\n- The paper is well-structured and easy to follow, making it accessible to a broad audience, and the topic is both relevant and timely given the current focus on web agents and LLMs.\n- The benchmark is publicly available, which is a significant step towards reproducibility and transparency in research.\n\n**Weaknesses**\n- The paper lacks a detailed explanation of the process of annotating the trajectories, particularly the criteria used by human annotators, which could affect the reliability and validity of the benchmark.\n- The experimental design is somewhat limited, focusing primarily on a single type of task, and lacks a broader range of task types and web environments to provide a more comprehensive evaluation.\n- The evaluation of LLM judges is primarily based on precision and recall, which may not fully capture the nuances of LLM performance, particularly in terms of the quality and relevance of their judgments.\n- The paper does not provide a detailed analysis of the differences between the proposed benchmark and existing benchmarks, nor does it discuss the scalability and applicability of the benchmark to different types of web agents and environments.\n- There are concerns regarding the representativeness and diversity of the web environments used in the benchmark, which might limit its generalizability to other types of web agents and tasks.\n\n**Questions**\n- Could the authors provide more details on the criteria used by human annotators when annotating trajectories? How were these criteria developed and validated?\n- How does the proposed benchmark differ from existing benchmarks, and what advantages does it offer in terms of evaluating LLM judges?\n- What are the limitations of the current benchmark, and how can it be improved to better capture the complexities of web agent tasks and LLM performance?\n- Can the authors provide more information on the scalability and applicability of the benchmark to different types of web agents and environments?\n- How representative are the web environments used in the benchmark, and what steps can be taken to improve their diversity and relevance to a broader range of web agent tasks?"
    },
    "fuBrcTH8NM": {
        "inference_review": "**Summary**\nThis paper introduces a progressive training method for constructing model families, specifically targeting the efficient expansion of model sizes from smaller to larger. The approach, involving training models progressively, aims to reduce the overall computational costs and improve training efficiency compared to traditional independent training methods. The methodology leverages a range of experiments, including those on the LLaMA model family, to demonstrate the efficacy of this approach. The results highlight potential performance and cost benefits, although the paper's claims and experimental setups have been critiqued for their clarity and robustness. The method's effectiveness is debated, with concerns about its applicability across diverse model architectures and the generalizability of its findings.\n\n**Strengths**\n- The paper is clearly written, easy to understand, and provides a thorough analysis of the progressive training method across different model sizes, which is crucial for model scaling.\n- The authors demonstrate the effectiveness of progressive training in terms of computational cost savings and comparable performance to independent training.\n- The introduction of the concept of \"model expansion\" and \"progressive training\" as a novel approach in the field of model scaling.\n- The paper includes a comprehensive set of experiments and analysis on the model family, specifically on the LLaMA model family, which supports the proposed methods.\n- The findings on the consistency of the model family are intriguing and have implications for model deployment in real-world applications.\n- The authors have made their code available, enhancing reproducibility and allowing for further exploration by the community.\n\n**Weaknesses**\n- The paper lacks a rigorous experimental setup, with limited model architectures and parameter sizes tested, which may not fully support the claimed advantages of progressive training.\n- The scope of the study is somewhat narrow, focusing only on the LLaMA model family and a limited set of tasks. A broader range of tasks and model architectures could strengthen the findings.\n- Some aspects of the paper, such as the experimental design and the interpretation of results, are unclear or not convincingly presented, which may affect the reliability of the conclusions.\n- The paper does not sufficiently address the potential limitations and challenges of the proposed method, particularly in terms of its applicability to different model families and its performance in high-resource settings.\n- There are concerns about the novelty of the proposed method, as it shares similarities with existing techniques like distillation and transfer learning.\n\n**Questions**\n- Can the authors provide more details on how the progressive training method was implemented in their experiments? Specifically, how were the parameters of the smaller models used as initializations for the larger models?\n- Could the authors clarify the experimental design, particularly how the maximum learning rate was adjusted and the rationale behind this approach?\n- How does the progressive training method compare with other model scaling techniques like LoRA or fine-tuning in terms of computational efficiency and performance?\n- What are the potential limitations of the progressive training method, especially when applied to different model families or in high-resource settings?\n- How does the model family's performance change with varying degrees of model expansion, and how does this impact the overall performance and efficiency of the training process?\n- Could the authors discuss the implications of their findings on the consistency of model behavior across different sizes, and how this affects model deployment in practical scenarios?"
    },
    "MsgdEkcLRz": {
        "inference_review": "**Summary**\nThis paper introduces Lawflow, a new dataset designed to capture the complex, iterative nature of legal workflows. Lawflow focuses on the process of drafting operating agreements for small business formations, encompassing various human interactions and legal considerations. The dataset is constructed using role-playing scenarios where law students engage in mock client meetings, followed by drafting the agreement. The paper examines the workflow process using a graph-based analysis, comparing human decisions with those of large language models (LLMs) to identify deviations and potential areas for AI assistance. This study aims to bridge the gap between current datasets that only capture single legal tasks and real-world legal workflows, which involve multiple steps, revisions, and complex decision-making processes.\n\n**Strengths**\n- The paper addresses a crucial and underrepresented area in the field of legal reasoning by creating a new dataset that captures the complexities and nuances of legal workflows.\n- The dataset construction is robust, leveraging role-playing scenarios and a sophisticated graph-based analysis to capture human legal reasoning.\n- The study is well-grounded in its analysis of the workflow, highlighting the differences between human and LLM decision-making processes and providing insights into the limitations of LLMs in handling legal workflows.\n- The paper is well-written, making it accessible and easy to follow, with clear presentation of the methodology and results.\n- The research is original, presenting a novel perspective on the role of AI in legal workflows and offering a unique perspective on the challenges AI faces in such environments.\n\n**Weaknesses**\n- The paper lacks a detailed explanation of the dataset's construction, particularly the role-playing scenarios and the graph-based analysis.\n- There is insufficient comparison with existing datasets and benchmarks, which could provide a better context and highlight the unique contributions of this new dataset.\n- The paper does not adequately address the limitations of the dataset and the methodology, which is necessary for a comprehensive evaluation of the study's findings.\n- The experimental design and data analysis sections are unclear, particularly how the graphs were constructed and how the metrics were calculated, which could lead to confusion about the results.\n- The paper could benefit from additional examples and detailed case studies to better illustrate the findings and provide more concrete evidence for the claims made.\n\n**Questions**\n- Can the authors provide more details on the construction of the dataset, including the specifics of the role-playing scenarios and the graph-based analysis used?\n- How does this dataset differ from existing datasets in terms of the complexity and depth of legal workflows it captures?\n- Could the authors clarify the methodology used in the experimental design and data analysis, particularly regarding the construction of graphs and the calculation of metrics?\n- What are the limitations of the dataset, and how do these impact the validity and reliability of the findings?\n- How can the findings of this study be applied to real-world legal scenarios, and what implications do they have for the development of AI systems in the legal domain?\n- Can the authors elaborate on the specific metrics used and how they were calculated to ensure a robust and accurate analysis of the data?"
    },
    "z9SbcYYP0M": {
        "inference_review": "**Summary**\nThe paper introduces a method for editing personality traits in large language models (LLMs) using a probing-based approach. This method involves training a classifier to detect specific personality traits and then employing adversarial training to alter the traits. The paper evaluates this method using the PersonalityEdit benchmark and demonstrates its effectiveness by reducing the correlation between personality traits and the generated text. The approach has been tested on different LLMs, including LLaMA, PaLM, and Vicuna, with varying degrees of success. The paper also discusses the potential of this method to improve the robustness of LLMs against adversarial attacks and to enhance their general capabilities.\n\n**Strengths**\n- The paper addresses an important and relevant topic in the field of AI, particularly the manipulation of personality traits in large language models (LLMs), which is crucial for understanding the ethical implications and potential applications of these models.\n- The methodology proposed is novel, combining probing-based and adversarial techniques to alter personality traits, which adds a fresh perspective to existing methods.\n- The paper is well-written, making it accessible and easy to follow, which enhances its readability and comprehension.\n- The experiments are comprehensive, covering various LLMs and tasks, and the results are consistent with the stated goals, providing empirical evidence to support the claims made in the paper.\n- The concept of layer-wise editing of personality traits is innovative, and the proposed method shows potential for improving the robustness of LLMs against adversarial attacks, which is a significant contribution to the field.\n\n**Weaknesses**\n- The paper lacks a clear definition of personality traits, which is essential for understanding the scope and implications of the proposed method.\n- The novelty of the method is questionable as similar techniques have been previously explored in other studies, and the paper does not adequately discuss or differentiate its approach from these existing methods.\n- There is a lack of thoroughness in the experimental design, particularly in the evaluation of the proposed method. The paper does not provide sufficient evidence that the changes in personality traits are not due to changes in general capabilities, nor does it address potential biases in the datasets used.\n- The paper could benefit from a more detailed explanation of the adversarial training process, including how the method was implemented and what specific parameters were used.\n- The writing quality and organization of the paper need improvement, with sections and figures not being consistently referenced or explained, which could confuse readers.\n\n**Questions**\n- 1. Can you provide a clear definition of personality traits and explain how the proposed method specifically addresses these traits?\n- 2. How does the proposed method differ from existing methods, such as those mentioned in the references, and what are the advantages of your approach?\n- 3. In the experimental section, how are the edits applied to the LLMs, and what specific parameters were used in the adversarial training process?\n- 4. Could you clarify the experimental setup, particularly the evaluation metrics and datasets used, and address the concerns regarding the potential biases and limitations of these datasets?\n- 5. Can you elaborate on the adversarial training process, including the specific steps taken and the parameters used to ensure the effectiveness of the method?\n- 6. How does the proposed method address the ethical concerns related to manipulating personality traits in LLMs, and what are the potential societal impacts of this research?"
    },
    "zTKYKiWzIm": {
        "inference_review": "**Summary**\nThe paper introduces GenerationPrograms, a modular generation framework aimed at enhancing attribution quality in text generation tasks. It employs a two-stage process: first, generating a program plan comprising modular text operations tailored to the query, and second, executing these operations to produce the final response. This approach enables the model to explicitly track source inputs at each modular step, facilitating clear attribution and interpretability. The framework is evaluated using datasets from the long-form QA tasks, ASQA and LFQA, and shows significant improvements in attribution quality over baseline methods. However, concerns about the generalizability of the approach and its scalability to longer contexts are raised, alongside issues of program complexity and the method's practical applicability.\n\n**Strengths**\n- The paper introduces a novel approach to generation and attribution tasks, proposing a modular generation framework that enables better attribution quality and interpretability.\n- The paper is well-structured and clearly written, making it easy for readers to follow the concepts and methodologies presented.\n- The evaluation is extensive and covers various tasks, providing comprehensive results that demonstrate the effectiveness of the proposed methods.\n- The paper addresses the important issue of attribution quality in text generation, which is crucial for responsible AI systems.\n\n**Weaknesses**\n- The paper lacks a detailed comparison with recent relevant works such as \"Unsupervised Attribution\" and \"Self-Citation Generation,\" which could provide a more comprehensive context and justification for the proposed methods.\n- There are concerns about the generalizability of the proposed methods, particularly in scenarios involving longer contexts, which are not well addressed.\n- The paper does not sufficiently discuss the scalability and practicality of the proposed methods, particularly in terms of handling longer contexts and more complex generation tasks.\n- The complexity of the programs generated by the model could limit the interpretability and effectiveness of the attribution process.\n- The paper does not provide a detailed explanation of how the proposed methods can be adapted to other text generation tasks beyond the ones tested.\n\n**Questions**\n- Can the authors provide more details on how the proposed methods can be adapted to other text generation tasks beyond long-form QA?\n- How does the model handle longer contexts, and what strategies are employed to address the issues related to scalability and practicality?\n- Can the authors clarify the potential limitations of the program-based approach in terms of interpretability and attribution quality?\n- How does the proposed framework perform in terms of citation accuracy and recall, especially when compared to other existing methods like Self-Citation Generation?\n- What are the potential issues with the complexity of the programs generated by the model, and how can these be mitigated to improve the effectiveness of the attribution process?"
    }
}