{
    "0zxugBcgF5": {
        "inference_review": "**Summary**\nThe paper addresses the overoptimization issue in Reinforcement Learning from Human Feedback (RLHF) by proposing a method that utilizes importance weighting to address the distribution shift caused by the change in policy during RL training. This method, termed Off-Policy Corrected Reward Modeling (OCRM), aims to provide a consistent estimate of the reward model's parameters, potentially leading to improved policy optimization. The approach involves multiple training stages with each stage training a reward model on data from the current policy and then performing multiple policy updates. Experiments are conducted using summarization and chatbot datasets, demonstrating the method's effectiveness compared to standard RLHF methods and baselines. However, the paper lacks extensive empirical validation and a clear demonstration of its practical applicability in real-world scenarios.\n\n**Strengths**\n- The paper is well-written, clearly articulates the motivation and problem, and provides a detailed explanation of the proposed method, including a thorough discussion of the theoretical underpinnings.\n- The topic of addressing overoptimization in RLHF is highly relevant and of significant practical importance.\n- The proposed method leverages importance weighting to account for distribution shifts, which is a novel approach in the context of RLHF.\n- The experimental results demonstrate that the proposed method outperforms baseline methods on summarization and chatbot datasets.\n- The paper provides a comprehensive review of related work and offers a detailed discussion of the trade-offs involved in different methods, such as the balance between bias and variance in importance weighting.\n\n**Weaknesses**\n- The paper lacks empirical validation on more complex or realistic tasks, which may not accurately reflect the method's performance in real-world scenarios.\n- The experimental results are not sufficiently robust, with small standard deviations and limited evaluation metrics.\n- There is a lack of clarity and detail in the explanation of the method, particularly in how the importance weights are calculated and how the method integrates with existing RLHF frameworks.\n- The paper does not provide a clear motivation for the specific choice of importance weighting, nor does it discuss potential risks or limitations of the proposed method.\n- The paper does not adequately address the issue of overfitting, which is critical in RLHF applications where data quality is often poor.\n- The paper's presentation of results is not clear, and it would benefit from more visual aids like plots and tables to enhance understanding.\n\n**Questions**\n- How does the proposed method perform on more complex or realistic tasks, and what are the empirical results?\n- Can you provide more details on how the importance weights are calculated and how the method integrates with existing RLHF frameworks?\n- What are the specific risks and limitations associated with the proposed method, and how do these impact the practical application of the method?\n- How does the proposed method address the issue of overfitting, particularly in scenarios where data quality is poor?\n- What are the practical implications of the proposed method, and how does it compare to existing methods in terms of computational efficiency and practical implementation?"
    },
    "2H85485yAb": {
        "inference_review": "**Summary**\nThe paper explores the role of context in the latent spaces of Large Language Models (LLMs) through the lens of truth-value judgment. By employing different probing methods and error scores, the authors aim to understand how contextual information influences the representation of truth values within LLMs. The study introduces a novel error score and utilizes causal intervention experiments to analyze the impact of context on truth-value directions. Despite the novel contributions, the paper suffers from issues such as unclear definitions and assumptions, limited contextual analysis, and lack of comprehensive comparative analysis against existing methods.\n\n**Strengths**\n- The paper introduces novel error scores and conducts a thorough analysis of the impact of context on the representation of truth values in LLMs, which is a critical and interesting area of research.\n- The study employs causal intervention experiments to investigate the causal effects of premise representations on the truth-value judgments of hypotheses, demonstrating a thoughtful approach to understanding the dynamics of LLMs.\n- The authors provide a detailed description of the error scores and their construction, which is a significant contribution to the field of LLM interpretability.\n\n**Weaknesses**\n- The paper suffers from unclear definitions and assumptions, particularly regarding the nature and impact of the context on the truth-value representation, which may confuse readers and undermine the credibility of the findings.\n- The analysis is limited to a small number of LLMs, and the results may not be generalizable to other models or different contextual settings.\n- The paper lacks a comprehensive comparative analysis with existing methods, which is necessary to establish the novelty and effectiveness of the proposed error scores and methods.\n- The causal analysis conducted in the paper is restricted to only a few layers, and the study does not explore how context influences the entire latent space of LLMs, which could provide deeper insights into the model's behavior.\n- The experimental design and presentation of results are not clear or well-organized, making it difficult for readers to follow the arguments and understand the implications of the findings.\n- The paper does not address the potential biases introduced by the use of error scores, which could affect the validity of the conclusions drawn from the analysis.\n\n**Questions**\n- Could the authors clarify the assumptions made about the context in the study, particularly the nature and impact of the context on the truth-value representation?\n- How do the authors plan to address the limitations of the current study, such as the small number of LLMs analyzed and the lack of comprehensive comparative analysis with existing methods?\n- In light of the causal analysis conducted, what are the implications for the entire latent space of LLMs, and how does the context influence the model's behavior at different layers?\n- Could the authors provide more details on the experimental design and results, particularly how the error scores are constructed and how they relate to the truth-value representation in LLMs?\n- Given the potential biases introduced by the use of error scores, how do the authors plan to address these biases in future studies to ensure the validity of the conclusions drawn from the analysis?"
    },
    "2YdSsi0bxK": {
        "inference_review": "**Summary**\nThe paper introduces a novel method named SUV for selectively unlearning copyrighted content in large language models (LLMs) using a Direct Preference Optimization (DPO) approach. The method targets verbatim copyrighted content, aiming to prevent LLMs from memorizing such material while preserving overall utility. The approach involves constructing a dataset of copyright infringement instances, using it to unlearn the content through DPO, and integrating gradient projection and Fisher information regularization to mitigate performance degradation on unrelated tasks. Experimental validations on a large-scale dataset of 500 copyrighted books show that SUV significantly reduces verbatim memorization with minimal impact on unrelated tasks' performance. The paper also explores scalability and efficacy through extensive experiments, although it has been criticized for its limited novelty, unclear methodology, and potentially biased evaluation metrics.\n\n**Strengths**\n- The paper introduces an innovative method for selective unlearning of copyrighted content in LLMs, which is a timely and highly relevant topic given the increasing usage of LLMs in various applications.\n- The approach is well-motivated and clearly explained, with a comprehensive experimental setup that demonstrates the effectiveness of the proposed method.\n- The authors have conducted extensive experiments on a large-scale dataset, showing that the method is scalable and effective in reducing verbatim memorization with minimal impact on unrelated tasks.\n- The paper is well-written, with clear explanations of the methodology and results, making it easy to follow and understand.\n\n**Weaknesses**\n- The paper lacks sufficient novelty, as similar approaches to selective unlearning of copyrighted content have been previously explored.\n- The methodology section is unclear and could benefit from additional details and ablation studies to substantiate the proposed methods.\n- The evaluation metrics used may be biased towards the proposed method, as the baselines might not be comparable due to differences in dataset size and model scale.\n- The paper does not adequately address the ethical implications of selectively unlearning copyrighted content, which could be a significant concern in real-world applications.\n- There is a lack of detailed information on the experimental setup, such as the size of the datasets used, the models employed, and the hyperparameters.\n- The paper does not provide a comprehensive comparison with existing methods, which makes it difficult to gauge the effectiveness of the proposed approach relative to other state-of-the-art methods.\n\n**Questions**\n- Could the authors clarify the novelty of the proposed method in comparison to existing works in the field?\n- What are the detailed hyperparameters used in the experiments, and how were the baselines selected and compared?\n- How were the evaluation metrics chosen, and could alternative metrics that are less biased be considered?\n- What are the ethical implications of selectively unlearning copyrighted content, and how does the proposed method address these concerns?\n- Can the authors provide more details on the experimental setup, such as the size of the datasets and models used, and the specific implementation of the proposed method?\n- How does the proposed method compare with other state-of-the-art methods in terms of effectiveness and scalability?"
    },
    "BM192Ps5Nv": {
        "inference_review": "**Summary**\nThe paper explores the quantization of reasoning large language models (LLMs) to enhance efficiency without compromising performance. It evaluates various quantization methods across different reasoning benchmarks and models, highlighting the impact of quantization on reasoning models and identifying the best quantization configurations for specific tasks and model sizes. The study focuses on quantizing weights, activations, and KV cache to achieve lossless or near-lossless performance. The findings suggest that quantization can effectively reduce model size and latency, although there are significant challenges in scaling and quantizing reasoning models due to the increased complexity of reasoning tasks. The paper also discusses the differences in quantization performance across various model families and tasks, indicating that model size and task difficulty are critical factors in determining the effectiveness of quantization.\n\n**Strengths**\n- The paper addresses an important and timely topic, as quantization can significantly improve the efficiency of reasoning large language models (LLMs).\n- It is well-written, well-organized, and easy to follow, making the content accessible and understandable.\n- The research is thorough, covering a wide range of quantization techniques across various reasoning benchmarks and models, which helps in understanding the quantization of reasoning models in depth.\n- The paper includes detailed analysis and ablation studies, providing insights into the effects of quantization on reasoning models and the choice of quantization techniques based on model characteristics and tasks.\n- The experimental results are presented clearly and comprehensively, aiding in the evaluation of quantization methods and their impact on different model sizes and reasoning tasks.\n\n**Weaknesses**\n- The paper lacks a clear motivation and introduction to the problem, particularly regarding why quantization is necessary for reasoning LLMs and what are the primary challenges in this area.\n- It does not sufficiently explore the impact of quantization on inference latency, a critical aspect of practical deployment.\n- The experimental setup has several limitations, including the use of outdated benchmarks and the absence of a broader range of models and quantization techniques.\n- The paper does not provide a comprehensive discussion on the trade-offs between model performance and quantization efficiency, which is crucial for practical applications.\n- There is a lack of clarity in the presentation of results, particularly in the tables and figures where the data is not consistently labeled or explained, making it difficult to interpret the findings accurately.\n\n**Questions**\n- Can you provide a clearer motivation and introduction to the problem of quantizing reasoning LLMs, particularly highlighting the challenges and benefits of this approach?\n- How do the quantization techniques affect the inference latency of the models, and could you include latency measurements in your experiments?\n- Why were specific benchmarks like MATH-500 chosen, and how do these benchmarks compare to others in terms of complexity and representativeness?\n- Could you expand the range of models and quantization techniques tested in your experiments to provide a more comprehensive evaluation of quantization methods for reasoning LLMs?\n- In the presentation of results, could you improve the clarity and consistency of the data labeling and explanations to facilitate better understanding of the findings?\n- What are the implications of the observed trends in quantization performance across different model families and tasks, and how can these findings guide the selection of appropriate quantization techniques in practice?"
    },
    "exW2SFJK4H": {
        "inference_review": "**Summary**\nThe paper introduces a dynamic framework designed to test the robustness of unlearning methods in large language models (LLMs) using complex, structured queries. This framework aims to evaluate the efficacy of unlearning by generating dynamic, semantically equivalent probes that vary in query difficulty, thereby assessing the model's resilience to multi-hop reasoning and entity aliasing. The evaluation primarily focuses on the model's ability to handle single-hop and multi-hop queries, demonstrating that unlearning methods can fail to effectively remove targeted knowledge, especially in multi-hop settings. The paper also conducts an analysis of model activations, suggesting that unlearning primarily disrupts dominant computation pathways while leaving less affected pathways intact.\n\n**Strengths**\n- The paper introduces a novel dynamic framework for evaluating unlearning in large language models, which is a critical and under-explored topic in the field.\n- The proposed framework is dynamic and structured, enabling the automatic generation of complex and semantically equivalent queries, which is more efficient and effective compared to traditional benchmarks.\n- The paper is well-written, clear, and easy to follow, making it accessible to a broad audience.\n- The approach is innovative in its use of knowledge graphs and multi-hop queries to assess unlearning robustness, which is a significant advancement over previous methods.\n- The experiments are well-designed, comprehensive, and effectively demonstrate the potential of the framework in evaluating the effectiveness of unlearning methods.\n\n**Weaknesses**\n- The paper lacks a detailed comparison with existing unlearning benchmarks, which could help in understanding the novelty and advantages of the proposed framework over traditional methods.\n- The experimental setup, particularly the selection of unlearning methods and evaluation metrics, is not thoroughly justified, raising concerns about the breadth and depth of the evaluation.\n- The paper does not discuss the potential limitations or challenges of the framework, such as its applicability to different types of unlearning methods or its scalability to larger models.\n- There is a lack of clarity in the description of the framework's implementation, particularly in how the knowledge graph is constructed and how multi-hop queries are generated.\n- The paper does not discuss the potential biases or challenges associated with the dynamic nature of the framework, such as the impact of initial data on the knowledge graph and the potential for query bias.\n- The analysis of model activations is limited, and it does not provide a comprehensive explanation of why multi-hop queries are more effective at recovering forgotten information.\n\n**Questions**\n- How does the proposed framework compare to existing unlearning benchmarks in terms of coverage, evaluation metrics, and overall effectiveness?\n- Can you elaborate on the selection of unlearning methods and evaluation metrics used in the experiments? What are the specific criteria for evaluating the effectiveness of unlearning methods?\n- How does the framework handle different types of unlearning methods, such as gradient reversal and weight pruning? What are the challenges and limitations of applying the framework to these methods?\n- Could you provide more details on how the knowledge graph is constructed and how multi-hop queries are generated within the framework?\n- How does the framework address potential biases or challenges associated with its dynamic nature, such as the impact of initial data on the knowledge graph and the potential for query bias?\n- Can you provide a more detailed explanation of the analysis of model activations, particularly why multi-hop queries are more effective at recovering forgotten information?"
    },
    "fQcUZMPIvu": {
        "inference_review": "**Summary**\nThe paper introduces AgentRewardBench, a benchmark aimed at evaluating LLM judges' capabilities in assessing web agent trajectories. The benchmark features 1302 trajectories across 5 diverse web environments, each annotated by expert evaluators to gauge success, side effects, and repetitive actions. The paper explores various LLM judges, comparing them against expert judgments and traditional rule-based methods. The findings suggest that LLM judges, while potentially useful, exhibit significant discrepancies in performance across different benchmarks and scenarios. Despite the potential utility, the paper is critiqued for its limited novelty, as it primarily adapts existing methods, and for the lack of a clear methodology for choosing LLM judges. Moreover, the paper is seen as a collection of prior works rather than a significant advancement in the field.\n\n**Strengths**\n- The paper provides a comprehensive and detailed analysis of various LLM judges, evaluating their effectiveness across different tasks and environments, which is crucial for understanding their strengths and limitations.\n- The proposed AgentRewardBench is a valuable resource for the community, allowing for a more accurate and standardized evaluation of LLM judges, which could lead to improvements in future LLM-based web agent research.\n- The paper is well-organized and written, making it accessible and easy to follow, which is essential for disseminating research findings effectively to a broad audience.\n- The use of expert annotations and a well-defined annotation process enhances the credibility and reliability of the evaluations conducted, which is vital for establishing trust in the results.\n\n**Weaknesses**\n- The novelty of the paper is somewhat limited as it primarily adapts existing methods to a new dataset, without introducing significant new methodologies or insights that could advance the field further.\n- The selection criteria for LLM judges are not clearly defined, which raises concerns about the fairness and comparability of the evaluations.\n- The paper lacks a comprehensive analysis of the discrepancies between LLM judges and human annotations, which is critical for understanding the effectiveness and potential biases of LLMs in these evaluations.\n- The experimental setup and comparisons made are not sufficiently robust to draw definitive conclusions, especially in terms of the relative merits of different LLM judges.\n- The presentation of the results could be improved for better clarity and readability, particularly in terms of summarizing the findings and distinguishing between different types of evaluation metrics.\n\n**Questions**\n- How were the LLM judges chosen, and what were the specific criteria used to select these models for evaluation?\n- Can the authors provide a more detailed analysis of the discrepancies between LLM judges and human annotations, particularly in terms of the reasons behind these discrepancies?\n- Given the differences in performance observed across various benchmarks, how do the authors justify the use of a single metric (e.g., precision) to evaluate all LLM judges?\n- What is the rationale behind the decision to not include more recent LLM models like GPT-4 in the evaluation, and how might their inclusion affect the results and conclusions drawn?\n- How can the paper be improved to better address the concerns regarding the novelty and the experimental setup of the study?\n- Could the authors elaborate on the potential applications and broader impacts of this research, particularly in terms of its practical utility for the development and deployment of web agents?"
    },
    "fuBrcTH8NM": {
        "inference_review": "**Summary**\nThe paper introduces a novel approach named \"progressive training\" aimed at training model families from smaller to larger models, thereby reducing the computational cost and enhancing model performance. This method leverages model expansion techniques and adjusts the learning rate based on model size, showing improvements in efficiency and performance over traditional independent training methods. The authors conducted extensive experiments to validate these claims, though the paper is criticized for its lack of clarity, insufficient experimental details, and inadequate comparison with existing methods. Furthermore, the paper's claims regarding the reduction in computational costs and model performance improvements are not convincingly substantiated by the data presented.\n\n**Strengths**\n- The paper introduces a novel approach to model training that potentially reduces the total training cost of constructing a model family by up to 25% compared to training each model independently.\n- The proposed method is innovative as it uses model expansion techniques to incrementally train models from smaller to larger sizes, potentially offering a significant reduction in training time.\n- The paper is well-written and easy to understand, making complex concepts accessible to a broader audience.\n- The method allows for more consistent behavior across different model sizes, which could be beneficial in various applications such as speculative decoding.\n\n**Weaknesses**\n- The paper lacks sufficient details on experimental setups, including the specific model architectures, hyperparameters, and data used, making it difficult to replicate the results or understand the underlying mechanisms.\n- The claim that the progressive training method reduces computational costs by up to 25% is not convincingly supported by the data presented, and the paper does not provide a clear comparison to existing methods.\n- The paper does not provide sufficient statistical evidence to support the claimed improvements in model performance, and the experiments are limited to a small number of model sizes (1B to 8B).\n- The writing quality could be improved, particularly in the introduction where the paper could benefit from more specific and detailed explanations of the contributions and motivations behind the proposed method.\n- The paper does not address how the method scales to larger model families or how it performs on different types of tasks, which could limit its applicability and understanding of its effectiveness.\n\n**Questions**\n- Can the authors provide more details on the experimental setup, including the specific model architectures, hyperparameters, and data used, to help replicate the results?\n- How does the proposed method compare to other existing methods in terms of computational cost reduction and model performance improvement?\n- Can the authors provide more statistical evidence to support the claimed improvements in model performance, such as confidence intervals or error bars?\n- How does the method perform on different types of tasks and larger model families, and what are the implications for its scalability and applicability?\n- Could the authors clarify the writing quality issues mentioned, particularly in the introduction, and provide a more detailed explanation of the contributions and motivations behind the proposed method?"
    },
    "MsgdEkcLRz": {
        "inference_review": "**Summary**\nThe paper introduces LawFlow, a dataset and tool designed to capture legal workflows by focusing on the end-to-end decision-making processes involved in drafting operating agreements for small business formation. It distinguishes itself from existing datasets by modeling the iterative and adaptive nature of legal reasoning, showcasing how humans and LLMs approach this complex process. The dataset, comprising 10 finalized scenarios, is designed to be accessible to researchers, providing valuable insights into the differences between human and LLM workflows. Despite its innovative approach and the potential for advancing legal AI research, the paper faces criticism for its limited scope, lack of diversity in tasks, and insufficient comparative analysis with existing datasets.\n\n**Strengths**\n- The paper is well-structured, making it easy to follow, and provides clear explanations, which enhance its accessibility to a broader audience.\n- The dataset is described as novel and valuable, highlighting the importance of end-to-end legal workflows in legal reasoning and AI research.\n- The paper's focus on the differences between human and LLM workflows provides valuable insights, showing that humans adapt and revise their workflow more frequently than LLMs, which follow a more rigid, linear approach.\n- The authors conducted thorough experiments to compare human and LLM workflows, demonstrating a robust methodological approach.\n- The dataset is open-source, which can facilitate further research and development in legal AI.\n\n**Weaknesses**\n- The paper's scope is too narrow, focusing solely on drafting operating agreements for small business formation, which may limit its generalizability to other legal tasks.\n- The diversity of tasks included in the dataset is insufficient, making it hard to assess the dataset's broader applicability.\n- The paper lacks a comparative analysis with existing datasets, such as those from the Legal Information Retrieval Dataset (LIRD) or the Legal Entity Dataset, which could provide a stronger context and validate the findings.\n- The experiments and analyses conducted are not adequately detailed in the main paper, and critical details are relegated to the appendix, making it difficult for readers to fully grasp the methods and results.\n- The paper does not sufficiently address the limitations of its dataset, particularly concerning the absence of a broader range of legal tasks and the potential for human bias in data collection.\n\n**Questions**\n- How does the LawFlow dataset compare with other existing datasets in terms of diversity, coverage, and applicability to a broader range of legal tasks?\n- Can you provide a more detailed explanation of the data collection process, especially concerning the potential biases introduced by human annotators?\n- What are the specific limitations of the LawFlow dataset, and how can these be addressed in future versions or extensions of the dataset?\n- Could the authors clarify the methodology used to construct the Human Task Plan and explain the rationale behind the exclusion of certain steps in the workflow?\n- How does the dataset's focus on drafting operating agreements for small business formation align with or diverge from the typical tasks encountered in real-world legal practice?"
    },
    "z9SbcYYP0M": {
        "inference_review": "**Summary**\nThis paper introduces a layer-wise probing framework to investigate the internal encoding of personality traits in Large Language Models (LLMs). The framework trains a linear classifier on each layer's output to detect personality traits, with the hypothesis that middle layers primarily encode personality information. The authors validate this by showing that personality editing can be achieved in these middle layers without impacting the overall performance of the LLM. The study further explores how personality traits are represented across various layers of LLMs and proposes a method for editing personality traits in LLMs. The paper provides a detailed analysis using several LLMs and benchmarks, though it faces criticism for its limited scope, unclear experimental setup, and potential lack of novelty.\n\n**Strengths**\n- The paper is well-structured and easy to follow, making it accessible to a broad audience.\n- The method for investigating personality encoding in LLMs is clearly presented, and the approach is simple and straightforward.\n- The paper includes a comprehensive analysis of the encoding of personality traits across multiple layers of LLMs, providing detailed insights into the model's inner workings.\n- The experiments are thorough, well-designed, and demonstrate the effectiveness of the proposed method in editing personality traits in LLMs.\n- The results section is particularly strong, offering a detailed breakdown of how personality traits are distributed across different layers of LLMs, which is beneficial for future research.\n\n**Weaknesses**\n- The paper lacks novelty in its approach, as the method of probing and editing personality traits in LLMs has been previously explored in other works.\n- There is an unclear and possibly inaccurate experimental setup regarding the evaluation of the LLM's performance after editing.\n- The paper could benefit from a more detailed discussion on the implications of the findings, such as how personality traits are encoded in the model and how this affects the overall performance of the LLM.\n- The presentation of the results could be improved, particularly in the tables and figures, which are sometimes difficult to interpret.\n- The paper could benefit from additional experimental results on more recent and diverse LLM models to validate the findings.\n- There is a lack of clarity in the experimental setup and the evaluation process, which could lead to confusion about the results and their implications.\n\n**Questions**\n- Could you clarify the experimental setup and the evaluation process used to assess the LLM's performance after editing?\n- How does the proposed method compare to existing methods in terms of effectiveness and efficiency?\n- What are the implications of the findings regarding the encoding of personality traits in LLMs, and how do these affect the overall performance of the model?\n- Can you provide more details on how the results were generated and why certain tables and figures are structured as they are?\n- How do the findings of this study align with or differ from previous research on personality encoding in LLMs?\n- Are there any plans to conduct additional experiments on more recent and diverse LLM models to validate the findings?"
    },
    "zTKYKiWzIm": {
        "inference_review": "**Summary**\nThe paper introduces \"GenerationPrograms,\" a novel method for enhancing attribution quality in large language models (LLMs) by generating executable programs that guide text operations. This method is showcased through experiments on long-form question-answering and multi-document summarization tasks, demonstrating improvements in attribution accuracy. The authors claim that the proposed method can produce verifiable and trustable outputs, which are crucial for ensuring the integrity of LLM-generated content. Despite these advantages, the paper's experimental setup is criticized for its simplicity and lack of comprehensive comparisons with state-of-the-art methods. Additionally, there are concerns about the scalability of the proposed method and its applicability to more complex tasks.\n\n**Strengths**\n- The paper addresses a critical issue in the generation of verifiable and trustworthy outputs from large language models (LLMs), which is essential for their adoption in practical applications.\n- The proposed method, GenerationPrograms, is novel and well-articulated, effectively leveraging the \"program-to-code\" approach to generate a program that outlines the operations for generating text, enhancing interpretability and verifiability.\n- Experimental results demonstrate that GenerationPrograms can significantly improve attribution quality, providing a clear and structured explanation of how and why the model's outputs are generated.\n- The paper is well-structured and easy to follow, providing a clear introduction and a logical progression of ideas, which makes it accessible to readers.\n- The method shows potential for use as a post-hoc attribution method, and the proposed approach to attribution is contributive, making it more interpretable and useful for understanding model decisions.\n\n**Weaknesses**\n- The paper lacks a thorough comparison with existing state-of-the-art methods, which could help establish the relative effectiveness and novelty of the proposed approach.\n- The experiments are limited to a single dataset, and the method's scalability and applicability to more complex tasks or different types of tasks such as summarization and question answering are not thoroughly explored.\n- There is insufficient discussion on the computational cost and practicality of implementing the program generation and execution steps, which is critical for understanding the real-world feasibility of the approach.\n- The paper's reliance on a single LLM for all experiments raises concerns about the generalizability of the results, and there is a lack of robustness analysis to demonstrate the method's effectiveness across different models and conditions.\n- Some reviewers have pointed out that the paper's method could be seen as overly simplistic and that the proposed solution might not address the fundamental issues of model attribution and faithfulness.\n\n**Questions**\n- Can the authors provide a more detailed comparison with existing attribution methods and discuss how the proposed method differs or improves upon these approaches?\n- How does the method scale with different types of tasks, and can the authors provide examples or experiments demonstrating the method's effectiveness in various contexts?\n- Is the program generation step computationally expensive, and how does the method perform when the input context is longer than a single sentence?\n- Can the authors clarify how the proposed method would handle situations where the input context is longer than the maximum context length of the LLM?\n- What are the specific benefits of using a modular program generation approach compared to other methods, and how does this approach improve attribution quality and model interpretability?\n- How does the proposed method ensure that the generated output is faithful to the original input, and what mechanisms are in place to prevent hallucinations or incorrect attributions?"
    }
}