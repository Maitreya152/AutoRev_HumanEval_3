{
    "5xwyxupsLL": {
        "inference_review": "**Summary**\nThe paper introduces PipeFusion, a new parallelism method designed to enhance the inference performance of diffusion models by leveraging temporal redundancy and patch-level pipeline parallelism. This approach is particularly tailored for diffusion transformers and aims to reduce inference latency and memory requirements. The method was evaluated across various diffusion models, showing that PipeFusion outperforms existing parallel methods in terms of latency. However, the novelty of the approach is questioned due to its similarities to existing methods like DistriFusion, and there are concerns about its general applicability to different diffusion models and its scalability.\n\n**Strengths**\n- The proposed method achieves good results, showing a reduction in latency compared to previous methods, and it is easy to implement.\n- The paper is well-organized and easy to follow, providing a comprehensive discussion of related work and presenting the problem clearly.\n- The method introduces a new patch-level pipeline parallelism approach that utilizes temporal redundancy to improve inference performance.\n- The use of temporal redundancy is well-motivated, and the proposed method is simple to implement, demonstrating a good trade-off between performance and memory requirements.\n- The method shows scalability and can be applied to different diffusion models, enhancing the applicability and flexibility of the approach.\n\n**Weaknesses**\n- The novelty of the proposed method is questioned due to its similarity to DistriFusion, raising concerns about the extent of its originality.\n- There is a lack of ablation studies and detailed experimental comparisons to fully demonstrate the advantages and effectiveness of the proposed method over existing methods.\n- The paper lacks a detailed discussion on the limitations and challenges of the proposed method, particularly regarding its scalability and application in diverse diffusion models.\n- The paper could benefit from more extensive experimental evaluations, including comparisons with more recent works and a broader range of diffusion models to better establish its efficacy and relevance.\n- The writing style and presentation could be improved to enhance the clarity and readability of the paper.\n\n**Questions**\n- Can the authors clarify the differences between PipeFusion and DistriFusion and explain the specific advantages of PipeFusion over DistriFusion?\n- How does the proposed method compare with other recent works in terms of latency and memory usage, particularly in scenarios with varying image resolutions?\n- How does PipeFusion perform in scenarios where temporal redundancy is minimal, such as in models with a small number of diffusion steps?\n- Could the authors provide more detailed experimental results and ablation studies to demonstrate the effectiveness of PipeFusion?\n- What are the primary limitations and challenges associated with the proposed method, and how might these be addressed in future work?\n- Are there plans to extend the evaluation of PipeFusion to more recent diffusion models or other types of generative models to demonstrate its applicability and scalability?"
    },
    "F20AfNqMq9": {
        "inference_review": "**Summary**\nThe paper introduces a novel Active Inference (AIF) agent that integrates a generative model with a policy network, enabling long-horizon predictions and guiding policy optimization. The proposed architecture combines a multi-step latent transition model and an integrated policy network within an active inference framework. This setup allows for the model to produce long-horizon roll-outs and supply differentiable signals to the policy during optimization. The paper presents a joint training algorithm that alternately updates the generative model and the policy network, and shows how the learned model can be leveraged during planning via gradient updates to the policy. The empirical evaluation demonstrates the effectiveness of the proposed approach in a realistic industrial scenario with delayed and long-horizon settings.\n\n**Strengths**\n- The paper addresses the limitations of active inference (AIF) agents in delayed environments and long-horizon settings, which are crucial for practical applications.\n- The novel integration of a generative model with a policy network within the active inference framework is a significant contribution, enabling the model to produce long-horizon rollouts and supply differentiable signals to the policy during optimization.\n- The paper presents a joint training algorithm that alternately updates the generative model and the policy network, enhancing the training efficiency and effectiveness of the proposed approach.\n- The empirical evaluation is robust, with results showing that the proposed approach is effective in a realistic industrial scenario with delayed and long-horizon settings.\n\n**Weaknesses**\n- The paper's explanation of the active inference framework and its underlying principles is insufficient, particularly for readers not familiar with active inference, which may hinder understanding of the paper's contributions and significance.\n- The paper lacks a detailed discussion on how the proposed approach compares with existing methods, specifically in terms of computational efficiency, scalability, and the handling of delayed environments and long-horizon settings.\n- There is insufficient comparison with other model-based RL algorithms, which could provide a better context for evaluating the performance and effectiveness of the proposed method.\n- The experimental setup and evaluation metrics are not sufficiently detailed, making it difficult to assess the robustness and generalizability of the proposed approach.\n- The paper suffers from poor presentation, with multiple typographical errors, unclear figures, and an overall need for improvement in writing clarity and organization.\n\n**Questions**\n- Can the authors clarify the differences between the proposed approach and existing methods, such as Dreamer, in terms of computational efficiency, scalability, and performance in delayed environments and long-horizon settings?\n- How does the proposed method compare with other model-based RL algorithms in terms of sample efficiency, learning speed, and robustness to hyperparameter choices?\n- Could the authors provide more details on the experimental setup, including the choice of evaluation metrics and the statistical significance of the results reported?\n- In Section 4.1, the paper mentions that the agent improves its preference score over time. However, it does not provide enough details on how the preference score is calculated or what this improvement means in practical terms. Can the authors clarify this point?\n- The paper mentions that the generative model continues to improve even after policy optimization stabilizes. What implications does this have for the overall performance and effectiveness of the proposed approach?"
    },
    "FSowNqrLpp": {
        "inference_review": "**Summary**\nThe paper explores the challenges of aligning strong models with weaker supervision, highlighting a phenomenon known as \"weak-to-strong\" (W2S) generalization. The authors introduce a novel approach, referred to as Eve, which leverages the strong model's internal knowledge and relies on a weaker model's evaluation for guidance. This method is contrasted with existing approaches like naive learning and refinement, and is tested on tasks such as summarization and instruction following. The authors argue that Eve provides a more robust and effective framework for model alignment under weak supervision conditions, showing better performance than other methods like Direct Preference Optimization (DPO).\n\n**Strengths**\n- The paper is well-organized and easy to follow, making the complex concepts and motivations clear and accessible.\n- The authors provide a thorough analysis and characterization of the overfitting problem in weak-to-strong (W2S) generalization, which is supported by both theoretical and empirical results.\n- The proposed method, Eve, demonstrates significant improvements over existing baselines, showing superior performance in various tasks.\n- The paper is well-written and provides a clear motivation for the research, contributing to a better understanding of the challenges and opportunities in the field.\n\n**Weaknesses**\n- The paper's presentation could be improved for better readability and understanding, particularly in the introduction and related work sections. Some definitions and explanations are missing or unclear.\n- The methodology used to train and evaluate the models, particularly the strong reference model, lacks clarity and detail. This includes the lack of a detailed description of the training and evaluation datasets, which hampers the reproducibility and understanding of the results.\n- The paper could benefit from a more rigorous evaluation, including comparisons with a broader range of baselines and additional experimental results to substantiate the claims.\n- There is an apparent contradiction in the paper regarding the use of KL regularization, which could be clarified to avoid confusion.\n- The paper could benefit from a more thorough discussion on the choice and implications of the hyperparameter β, as well as the impact of using a strong reference model in the proposed method.\n\n**Questions**\n- Could you clarify the definitions and notations used in the paper, particularly in Section 2.1 and Section 4?\n- How are the strong reference models and weak reference models trained? Can you provide more details on the training and evaluation datasets used?\n- Why does the paper use the strong reference model, and what is the rationale behind choosing this approach?\n- Can you provide additional experimental results or comparisons with more baselines to better support the claims and demonstrate the robustness of the proposed method?\n- How does the choice of hyperparameter β affect the results, and what are the implications of using a strong reference model in the proposed method?\n- Could you discuss the limitations of the proposed method and suggest potential future research directions?"
    },
    "H8fscnm6Xx": {
        "inference_review": "**Summary**\nThe paper introduces a novel approach for secure and decentralized training of neural networks called Unextractable Protocol Models (UPMs). UPMs utilize the sharded model setup to ensure that model shards held by participants are incompatible at different time steps, thereby preventing the unauthorized extraction of model weights. The authors apply random, invertible transforms to model weights at each pipeline boundary, maintaining the overall network function while ensuring incompatibility across different time steps. Experimental results on Qwen-2.5-0.5B and Llama-3.2-1B models show that the proposed method does not significantly degrade model performance, while adding minor latency, bandwidth, and GPU memory overhead. However, the paper faces criticism for insufficient evaluation on real-world decentralized training scenarios and for not addressing the practicality of the proposed approach under different attack scenarios.\n\n**Strengths**\n- The paper introduces a novel approach to ensure secure and decentralized training of neural networks, utilizing random, invertible transforms at pipeline boundaries to ensure incompatibility of model shards across different time steps.\n- The authors provide a thorough analysis of the potential for model extraction and propose a method to prevent it, which is critical in decentralized settings.\n- The method preserves the overall network function while ensuring that the weights of the model shards are incompatible at different time steps, maintaining the integrity of the model.\n- The paper is well-written and organized, making it easy to follow and understand the proposed method and its implications.\n\n**Weaknesses**\n- The paper does not sufficiently evaluate the practicality of the proposed method in real-world decentralized training scenarios, which limits the understanding of its effectiveness and efficiency.\n- The experimental results are based on a single machine simulation, which does not accurately reflect the performance in a distributed environment.\n- The method's efficacy against various types of attacks, including gradient-based attacks and Sybil attacks, is not adequately discussed or tested.\n- The assumption of a non-colluding majority in the participant group may not be realistic, as it is challenging to maintain such a level of cooperation in decentralized settings.\n- The paper lacks a discussion on the impact of the method on training dynamics and does not provide a detailed analysis of the overhead costs associated with the proposed method.\n- The method's scalability is not demonstrated, and the paper does not address how the method would perform with a large number of participants or in scenarios with a large model size.\n\n**Questions**\n- How does the proposed method perform in real-world decentralized training scenarios, and what are the expected overhead costs?\n- Can the authors provide a detailed analysis of the training dynamics and how the proposed method affects them?\n- How does the method perform against various types of attacks, including gradient-based attacks and Sybil attacks?\n- What is the scalability of the method, and how does it perform with a large number of participants or in scenarios with a large model size?\n- How does the method ensure the security of the model in cases where a non-colluding majority is not maintained?\n- Can the authors provide more details on the experimental setup and the specific models used in the experiments?"
    },
    "LPUr2CexmX": {
        "inference_review": "**Summary**\nThe paper introduces the Density Operator Expectation Maximization (DO-EM) algorithm, a novel framework for training density operator models on classical hardware. The method is derived from a quantum information projection problem and utilizes the Petz Recovery Map. This approach enables training on real-world datasets such as MNIST, showcasing a 40-60% reduction in Frechet Inception Distance over deep Boltzmann machines. However, the paper's novelty is questioned due to similarities with previous works like the Quantum Boltzmann Machine (QBM), and the experimental setup and theoretical justification of the proposed methods are criticized for lack of depth and clarity. The paper also suffers from several typographical and referencing errors which detract from its overall quality.\n\n**Strengths**\n- The paper provides a clear and well-articulated explanation of the DO-EM algorithm, which is a significant contribution to the field.\n- It offers a novel application of quantum mechanics to classical data, demonstrating a reduction in Frechet Inception Distance, which is a valuable advancement.\n- The paper is well-written and easy to follow, making it accessible to a wide range of readers.\n- The introduction of the Density Operator Expectation Maximization (DO-EM) algorithm is a notable achievement, enhancing the training capabilities of density operator models.\n- The paper effectively compares the proposed methods to previous works, highlighting the improvements in performance and efficiency.\n\n**Weaknesses**\n- The paper's contribution is limited by its lack of novelty, as it heavily relies on existing works such as the Quantum Boltzmann Machine (QBM), which is already established in the literature.\n- There is a significant gap in the theoretical explanation, particularly regarding the derivation of the QELBO and the minorant-maximization perspective. The paper fails to provide a clear and comprehensive theoretical justification for its claims.\n- The experimental setup is poorly executed, with insufficient details provided about the hardware and computational resources used, which raises concerns about the reproducibility and reliability of the results.\n- The paper contains several typographical and referencing errors, including incorrect citations and undefined variables, which detract from its overall quality and professional presentation.\n- The paper's use of quantum terminology and concepts is inconsistent and confusing, which may lead to misunderstandings or misinterpretations among readers.\n- The application of the DO-EM algorithm to real-world data, such as MNIST, is not convincingly demonstrated, and the paper lacks a rigorous comparison with classical models to substantiate the claimed advantages of the quantum approach.\n\n**Questions**\n- Can the authors clarify the specific contributions of this paper compared to existing works such as the Quantum Boltzmann Machine (QBM)?\n- How does the QELBO compare to classical ELBO, and what are the implications of this comparison for the performance and efficiency of the DO-EM algorithm?\n- Why were specific experimental conditions chosen for the MNIST dataset, and how do these conditions impact the validity and reliability of the results?\n- Could the authors provide more detailed theoretical explanations, particularly regarding the derivation of the QELBO and the minorant-maximization perspective?\n- How does the paper address the challenges of scaling the DO-EM algorithm to larger datasets, and what are the potential limitations or bottlenecks in this regard?\n- The paper references various quantum concepts and terminology; could the authors provide a more comprehensive explanation of these terms to clarify their use and significance in the context of the paper?"
    },
    "MjOf5qnEX7": {
        "inference_review": "**Summary**\nThe paper addresses offline reinforcement learning (RL) within average-reward Markov Decision Processes (MDPs), focusing on the challenges posed by distribution shifts and non-uniform coverage. It introduces a novel algorithm based on pessimistic discounted value iteration and a quantile clipping technique to handle these complexities, providing the first fully single-policy sample complexity bound. The approach is evaluated theoretically and via lower bounds, with the paper demonstrating that learning under certain conditions necessitates data beyond the stationary distribution of the target policy. The algorithm does not require prior parameter knowledge and is applicable to general weakly communicating MDPs, though it is limited to the tabular setting.\n\n**Strengths**\n- The paper addresses a significant and relatively underexplored topic in reinforcement learning, focusing on offline RL in average-reward MDPs, which is critical for real-world applications where data collection is expensive or dangerous.\n- The proposed algorithm is simple and intuitive, based on a pessimistic discounted value iteration procedure with a quantile clipping technique that enables the use of a sharper empirical-span-based penalty function.\n- The paper provides sharp guarantees depending only on the target policy, introducing a novel policy hitting radius, which is a significant advancement in the field.\n- The approach does not require prior knowledge of parameters, which is essential for practical applications where such knowledge is often lacking.\n- Theoretical analysis is robust, including a comprehensive set of theoretical results and lower bounds that establish the optimality of the proposed algorithm.\n\n**Weaknesses**\n- The paper's novelty is limited as it largely builds upon existing work such as the pessimistic value iteration approach from Li et al. (2023), and the quantile clipping technique seems to be a straightforward adaptation of previous ideas.\n- The paper's theoretical contributions are incremental, and it primarily improves upon existing bounds without introducing fundamentally new ideas.\n- The assumptions used in the paper are quite strong, and the practical implications of these assumptions are not adequately discussed.\n- The paper does not discuss the broader implications or the impact of the results on the field beyond the specific context of average-reward MDPs.\n- The experimental section is lacking, which is a significant drawback as empirical validation is crucial for establishing the effectiveness and applicability of the proposed algorithm.\n\n**Questions**\n- Could the authors provide more details on the quantile clipping technique and how it relates to previous works such as those by Jin et al. (2021) and Rashidinejad et al. (2022)?\n- Is there a specific reason why the algorithm's convergence is guaranteed only for the tabular setting? Could the authors consider extending the convergence guarantees to function approximation settings?\n- Can the authors discuss the assumptions made in the paper and their practical implications in more detail?\n- How do the authors envision the practical application of their algorithm, particularly in real-world scenarios where the assumptions might not hold?\n- Could the authors clarify the relationship between the paper's theoretical contributions and the overall novelty of the work?"
    },
    "REHjkmWdQL": {
        "inference_review": "**Summary**\nThe paper introduces the Feature Monosemanticity Score (FMS) to evaluate the monosemanticity of latent features in large language models (LLMs), particularly through the proposed Guided Sparse Autoencoders (G-SAE). The G-SAE is designed to condition latent representations on labeled concepts during training, aiming to enhance feature monosemanticity and improve interpretability, detection, and control of LLMs. The methodology involves a binary tree classifier to localize concepts and measure monosemanticity through capacity, local disentanglement, and global disentanglement. The authors demonstrate that G-SAE improves upon existing SAEs in monosemanticity, detection, and steering capabilities, although the experiments are limited to smaller models, and the novelty and significance of the proposed methods are questioned.\n\n**Strengths**\n- The paper is well-written and easy to follow, with clear descriptions and illustrations that aid understanding.\n- The authors introduce the Feature Monosemanticity Score (FMS) as a novel metric for measuring feature monosemanticity in latent representations, which is a significant contribution to the field.\n- The Guided Sparse Autoencoders (G-SAE) approach proposed in the paper is innovative, focusing on conditioning latent representations on labeled concepts during training to enhance feature monosemanticity.\n- The paper conducts extensive experiments that demonstrate the effectiveness of the G-SAE method, which outperforms existing SAEs in terms of monosemanticity, detection, and steering capabilities.\n- The motivation for the paper is clearly articulated, highlighting the importance of understanding and improving the internal dynamics of large language models (LLMs) for better interpretability and control.\n\n**Weaknesses**\n- The paper's focus on the G-SAE method and the FMS metric might limit its novelty and significance, as similar techniques are already used in other domains such as computer vision.\n- The experimental setup is limited in scope, primarily using small-scale models and datasets, which may not fully demonstrate the effectiveness and generalizability of the proposed methods.\n- The paper does not provide a comprehensive comparison with other recent works in the field, which could enhance the credibility and relevance of the findings.\n- There are concerns about the clarity and motivation behind certain components of the paper, such as the use of specific metrics and the relevance of the datasets used for training and testing.\n- The novelty of the FMS metric is questionable as it seems to be a recombination of existing metrics, and the motivation for using it in this context is not convincingly explained.\n\n**Questions**\n- Could the authors clarify the novelty and significance of the G-SAE method and the FMS metric in the context of existing works in the field?\n- What is the rationale behind choosing the specific datasets and metrics used in the study? How do these choices align with the research questions and objectives of the paper?\n- Can the authors provide more details on the experimental setup, including the selection of model sizes and the specific configurations used in the experiments?\n- How does the proposed G-SAE method compare with other recent works in terms of interpretability and control of LLMs?\n- Could the authors elaborate on the potential limitations of the G-SAE method and the FMS metric, especially in terms of their practical applications and scalability to larger models and datasets?\n- What are the implications of the findings for the broader research community, and how do they contribute to the advancement of the field?"
    },
    "tfbu0ITAez": {
        "inference_review": "**Summary**\nThe paper presents SIGMA, a novel method that enhances the quality of reasoning traces generated by Monte Carlo Tree Search (MCTS) by incorporating sibling nodes typically discarded in the process. This approach involves critiquing and revising these sibling nodes to refine the reasoning path, a technique called Sibling Guided Monte Carlo Augmentation. SIGMA operates on the selected path of an MCTS tree and iteratively refines each step by comparing it with its siblings, using a critique model to identify overlooked strengths and weaknesses. The paper demonstrates SIGMA’s effectiveness across various benchmarks, showing significant improvements in reasoning accuracy with less training data compared to existing methods. However, concerns were raised about the novelty of the approach, the clarity of the writing, and the experimental setup, particularly the use of multiple baselines and the fairness in comparison.\n\n**Strengths**\n- The paper introduces an innovative technique by leveraging sibling nodes from Monte Carlo Tree Search (MCTS) to refine the reasoning paths, which is a novel application in the field.\n- SIGMA is shown to outperform existing methods across multiple benchmarks, demonstrating its effectiveness in enhancing the quality of reasoning traces.\n- The paper is well-structured and clearly written, making it accessible and easy to follow, which facilitates understanding of the proposed method.\n- The method introduces a unique approach by critiquing and revising sibling nodes, contributing to the refinement of reasoning paths and potentially improving the performance of large language models (LLMs) in reasoning tasks.\n- The use of a critique model to analyze the reasoning paths and a revision model to refine them is a promising strategy that could be valuable for other applications beyond the current scope.\n\n**Weaknesses**\n- The paper's writing needs significant improvement for clarity and precision, particularly in defining and explaining technical terms and concepts.\n- There are several issues with the experimental setup, including the use of multiple baselines that are not clearly defined or explained, and inconsistent reporting of results across different tables and figures.\n- The paper lacks a detailed description of the experimental design, including the specifics of the datasets used, the number of runs performed, and the statistical significance of the reported results.\n- The comparison with existing methods is not thorough or fair, with some methods not being included in the comparisons, which undermines the validity of the claimed improvements.\n- There is a lack of novelty in the approach as the use of sibling nodes and the refinement process are not significantly new, and the method appears to be an extension of existing techniques rather than a groundbreaking innovation.\n\n**Questions**\n- Could you clarify the specific baselines used in the experiments and provide a more detailed description of the experimental setup, including the datasets and the statistical analysis of the results?\n- Why was the GPT-4 model not included as a teacher model in the experiments, and what were the reasons for choosing specific models for the critique and revision stages?\n- The paper should address the lack of clarity in the writing, particularly in the introduction and related work sections, to improve the readability and comprehension of the content.\n- Given the concerns about the novelty of the approach, could you elaborate on how SIGMA significantly advances the state-of-the-art in MCTS-based reasoning and what specific contributions it makes to the field?"
    },
    "vWaMUMrBpF": {
        "inference_review": "**Summary**\nThe paper introduces a novel metric termed \"local inconsistency,\" which aims to measure the sensitivity of a neural network's output to perturbations in the parameter space, computed from unlabeled data. This metric is theoretically grounded in the Fisher Information Matrix (FIM) and the loss Hessian. The authors propose Inconsistency-Aware Minimization (IAM), a method that incorporates local inconsistency into the objective function to enhance generalization. Despite its practical advantages, the paper suffers from several issues including lack of clarity in the definition and motivation of the local inconsistency metric, insufficient theoretical backing, and unclear advantages over existing methods like SAM. The experimental results, while showing some promise, are deemed insufficiently convincing due to limited scope and unclear presentation.\n\n**Strengths**\n- The paper introduces a new metric for measuring the sensitivity of a neural network's output to perturbations in the parameter space, which can be computed from unlabeled data, thereby enhancing practical utility.\n- The metric is theoretically grounded in the Fisher Information Matrix (FIM) and the loss Hessian, providing a solid foundation for its claims.\n- The paper is generally well-written, making it accessible to a broad audience and the experimental results are reasonable and show some promise.\n- The proposed Inconsistency-Aware Minimization (IAM) approach is novel and may offer a new perspective in optimizing neural networks, with the potential to improve generalization performance.\n\n**Weaknesses**\n- The paper suffers from a lack of clear definition and motivation behind the local inconsistency metric, with insufficient theoretical backing to support its claims.\n- The advantages of the local inconsistency metric over existing methods like Sharpness-Aware Minimization (SAM) are not clearly demonstrated, with some experiments showing that SAM performs better than IAM.\n- The experimental results are limited in scope, primarily focused on CIFAR-10 and CIFAR-100 datasets, which may not adequately represent the broader applicability of the proposed methods.\n- The paper is not very clear in its presentation, with many sections containing typos and unclear notation which could hinder understanding.\n- The method for computing the proposed metric is computationally expensive, which could limit its practical application in large-scale neural networks.\n- The paper lacks a rigorous analysis of the proposed metric's behavior in different scenarios and does not sufficiently explore its relationship with other known measures such as sharpness and disagreement.\n\n**Questions**\n- Could the authors clarify the motivation and definition of the local inconsistency metric in more detail?\n- How does the proposed metric relate to other existing metrics such as sharpness and disagreement, and what are its unique advantages?\n- Can the authors provide more comprehensive experimental results, including comparisons with other methods like SAM, to better demonstrate the effectiveness and applicability of the proposed methods?\n- What are the implications of the proposed metric's computational complexity, and are there potential ways to reduce it?\n- How does the proposed method perform on larger and more complex datasets, such as ImageNet?\n- Could the authors provide a more rigorous theoretical analysis of the proposed metric's behavior and its relationship with known measures of generalization?"
    },
    "YtsX7irxbq": {
        "inference_review": "**Summary**\nThe paper explores the differences in the behavior of Attention and State Space Models (SSMs) during training on the Multi-Query Associative Recall (MQAR) task. It compares various aspects such as training dynamics, the role of learning rates, scaling in width and depth, and the impact of architectural modifications. The study reveals that SSMs are more sensitive to learning rates, and while they scale well with width, attention models benefit more from scaling in depth. Additionally, the paper identifies that single-layer transformers exhibit similar training dynamics to induction heads in multi-layer models, while SSMs can solve MQAR even in a single-layer setting with appropriate tuning. The findings suggest a nuanced understanding of the capabilities and limitations of Attention and SSMs, although some aspects, like the relevance of the MQAR task and the comparisons with previous studies, are questioned.\n\n**Strengths**\n- The paper presents a clear and comprehensive overview of the MQAR task, providing a thorough introduction to the experimental setup and data analysis.\n- It demonstrates an extensive exploration of the effects of learning rate, width, and depth on MQAR performance, highlighting the differences between attention and SSM models in these areas.\n- The paper effectively utilizes various model architectures, including attention, state-space models, and their hybrids, to conduct comparative analyses and identify performance variations based on different model configurations.\n- The experimental approach is robust, with multiple runs and seeds used to ensure the reliability of the results, and the inclusion of ablation studies to dissect the performance impacts of various architectural components.\n- The paper is well-written, presenting the data and results in an understandable manner and providing detailed discussions on the implications of the findings.\n\n**Weaknesses**\n- The paper fails to discuss the relevance of the MQAR task in the context of the broader research community, specifically in the context of attention and SSM models, which could limit its impact and applicability.\n- The experimental scope is somewhat narrow, focusing primarily on a single task (MQAR), which might not adequately represent the diversity of tasks that SSM and attention models are typically applied to.\n- The paper does not sufficiently discuss the limitations of its findings, particularly the generalizability of the results to other tasks or under different conditions, which could limit the utility of the study for other researchers.\n- There is a notable absence of a detailed discussion on the implications of the results for the research community, which could have provided more context and direction for future research.\n- The manuscript contains some minor issues with notation and terminology, which could benefit from clarification to improve the readability and precision of the paper.\n\n**Questions**\n- Can the authors elaborate on the significance of the MQAR task in the context of current research and discuss its relevance to other tasks or applications that are commonly used to evaluate SSM and attention models?\n- How do the authors plan to address the limitations of their study, particularly in terms of generalizability to other tasks or different experimental conditions?\n- Could the authors provide more details on the experimental setup, such as the specific configurations and parameters used for each model, to enhance the reproducibility and understanding of the results?\n- In the context of the ablation studies, what specific architectural components were targeted, and what were the corresponding impacts on performance?\n- Could the authors clarify any discrepancies or potential issues noted in the paper, such as the use of different terminology or notation that might cause confusion?"
    }
}